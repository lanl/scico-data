{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82886a58",
   "metadata": {},
   "source": [
    "# Example: Data-Driven Priors for Inverse Problems\n",
    "\n",
    "In this example we will explore how to use CNN models as data-driven priors in a SCICO pipeline for performing computed tomography (CT) reconstruction.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Suppose that you are measuring CTs of similar objects and want to construct a pipeline to rapidly compute the reconstruction of a new measurement. In this case we we will use foams to develop such a reconstruction engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e3a00",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scico import plot\n",
    "\n",
    "plot.config_notebook_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c91bd",
   "metadata": {},
   "source": [
    "Run the next cell to generate and visualize one of such foams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88f605",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"  # set default colormap\n",
    "\n",
    "from xdesign import Foam, discrete_phantom\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7654)\n",
    "\n",
    "N = 256  # image size\n",
    "x_fm = discrete_phantom(Foam(size_range=[0.075, 0.0025], gap=1e-3, porosity=1), size=N)\n",
    "x_gt = x_fm / np.max(x_fm)\n",
    "x_gt = np.clip(x_gt, 0, 1.0)\n",
    "\n",
    "# Plot signal\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(x_gt)\n",
    "ax.set_title('Foam')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8afedf",
   "metadata": {},
   "source": [
    "This image shows one foam but data-driven pipelines assume that you have access to a wealth of relevant data. Let's generate several different foams to use in our CT reconstruction pipeline. Since we are interested in CT reconstruction, we need to generate both images and sinograms.\n",
    "\n",
    "\n",
    "SCICO provides CT projectors based on Python libraries such as ASTRA and SVMBIR. In this case we will use the\n",
    "ASTRA interface (see https://scico.readthedocs.io/en/latest/_autosummary/scico.linop.radon_astra.html).\n",
    "\n",
    "**Define an ASTRA SCICO CT projector assuming 45 equally spaced projections.** Normalize by the dimension of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfcfd4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scico.linop.radon_astra import TomographicProjector\n",
    "\n",
    "n_projection = 45  # number of projections\n",
    "angles = np.linspace(0, np.pi, n_projection)  # evenly spaced projection angles\n",
    "A = TomographicProjector(x_gt.shape, 1, N, angles) / N # Normalized Radon transform operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fe0e9",
   "metadata": {},
   "source": [
    "**Test your operator by computing the sinogram of the generated foam and plotting your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8727fda",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "sino = A @ x_gt\n",
    "\n",
    "fig, ax = plot.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "plot.imview(x_gt, title=\"Ground truth\", cbar=None, fig=fig, ax=ax[0])\n",
    "plot.imview(\n",
    "    sino,\n",
    "    title=\"Sinogram\",\n",
    "    cbar=None,\n",
    "    fig=fig,\n",
    "    ax=ax[1],\n",
    ")\n",
    "divider = make_axes_locatable(ax[1])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\n",
    "fig.colorbar(ax[1].get_images()[0], cax=cax, label=\"arbitrary units\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0856e4",
   "metadata": {},
   "source": [
    "Repeat the proces to generate at least 24 different (foam, sinogram) pairs. **Start by generating the foams.** We will show how you can distribute the sinogram generation for parallel computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430103e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfoams = 24\n",
    "foam_collection = np.zeros((nfoams, N, N))\n",
    "foam_collection[0] = x_gt\n",
    "for i in range(1, nfoams):\n",
    "    x_ = discrete_phantom(Foam(size_range=[0.075, 0.0025], gap=1e-3, porosity=1), size=N)\n",
    "    x_ = x_ / np.max(x_)\n",
    "    foam_collection[i] = np.clip(x_, 0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad09938",
   "metadata": {},
   "source": [
    "Run the next cell to plot the generated foams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7bb20",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrows = 4\n",
    "ncols = 6\n",
    "fig, ax = plot.subplots(nrows=nrows, ncols=ncols, figsize=(15, 10))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        plot.imview(foam_collection[i*ncols+j], cbar=None, fig=fig, ax=ax[i, j])\n",
    "    divider = make_axes_locatable(ax[i,j])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\n",
    "    fig.colorbar(ax[i,j].get_images()[0], cax=cax, label=\"arbitrary units\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545867c",
   "metadata": {},
   "source": [
    "Distributing the processing among GPUs in the same node is transparent in JAX, but in CPUs it only uses one core. The following commands force to have 8 core CPUs in the processing, but if GPUs are available, it will ignore the forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354ad3b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
    "platform = jax.lib.xla_bridge.get_backend().platform\n",
    "print(\"Platform: \", platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa49d6e",
   "metadata": {},
   "source": [
    "For purely jax functionality, a distributed processing can be computed via `jax.vmap`. However, the CT operator uses a python (not jax) library. In that case we can distribute the processing via `jax.lax`. Run the next cell to distribute the computation of the sinograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30e18b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sino_collection = jax.lax.map(lambda x : A @ x, foam_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c960350",
   "metadata": {},
   "source": [
    "Check the shape of the result. **Do you understand each of the dimensions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde0cc2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sino_collection.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a625fd",
   "metadata": {},
   "source": [
    "The shape corresponds to: (number of foams, number of projections, foam dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4e546",
   "metadata": {},
   "source": [
    "You are done with part 1. Please report back in the Webex chat: **done with part 1**.\n",
    "\n",
    "While you wait for others to finish, you could explore other SCICO linear operators that can be used to transform image data.\n",
    "\n",
    "ðŸ›‘ **PAUSE HERE** ðŸ›‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898fc27",
   "metadata": {},
   "source": [
    "# Solving a regularized reconstruction: Data-Driven Priors\n",
    "\n",
    "When there is an explicit representation of the forward model, the signal reconstruction can be posed as\n",
    "a regularized least squares problem\n",
    "\n",
    "$$ \\min_x \\| y - Ax \\|_2^2 + \\, \\lambda \\, r(x).$$\n",
    "\n",
    "For example, in the CT case, the forward model $A$ is the CT projector, the measurements are the sinograms $y$ and\n",
    "the solution $x$ represents the signal reconstruction. The constant $\\lambda > 0$, establishes the trade-off\n",
    "between the fidelity to the measurements and the regularization of the solution represented as $r(x)$.\n",
    "\n",
    "In many cases, the regularization is necessary to find a meaningful solution for ill-posed problems. The difficulty\n",
    "arises in specifying an efficient regularization criterion. Functions like TV are adequate for piece-wise constant\n",
    "solutions, but may be not expressive enough for more general cases. Frameworks like plug-and-play priors provide\n",
    "a convenient alternative for cases when denoisers implement appropriate artifact removal. A complementary\n",
    "strategy is to unroll the iterative optimization process and build a neural network model that can be trained\n",
    "end-to-end. This kind of pipeline also offers the benefit of rapid evaluation in deployment, although its\n",
    "performance will depend on the training data, as is usually the case in machine learning models.\n",
    "\n",
    "The following diagram illustrates the kind of ML structure we will be training for the CT reconstruction:\n",
    "\n",
    "![Unrolled end-to-end](unrolled.png \"Unrolled end-to-end\")\n",
    "\n",
    "In the diagram, the green blocks correspond to a denoiser, generally a residual convolutional neural network, and are trainable. The red blocks correspond to a data consistency block and use the forward and adjoint operators. We will be constructing and training one of such unrolled models available in SCICO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e922c",
   "metadata": {},
   "source": [
    "For this tutorial we will use the MoDL architecture.\n",
    "\n",
    "A class [flax.MoDLNet](../_autosummary/scico.learning.rst#scico.learning.MoDL)\n",
    " implements the MoDL architecture, which solves the optimization problem\n",
    "\n",
    "  $$\\mathrm{argmin}_{\\mathbf{x}} \\; \\| A \\mathbf{x} - \\mathbf{y} \\|_2^2 + \\lambda \\, \\| \\mathbf{x} - \\mathrm{D}_w(\\mathbf{x})\\|_2^2 \\;,$$\n",
    "\n",
    "where $A$ is a forward operator, in this case a CT projector, $\\mathbf{y}$ is a set of measurements, in this case a collection of sinograms, $\\mathrm{D}_w$ is the\n",
    " regularization (a denoiser), and $\\mathbf{x}$ is the set of reconstructed images.\n",
    "  The MoDL abstracts the iterative solution by an unrolled network where each iteration corresponds\n",
    "  to a different stage in the MoDL network and updates the prediction by solving\n",
    "\n",
    "  $$\\mathbf{x}^{k+1} = (A^T A + \\lambda \\, \\mathbf{I})^{-1} (A^T \\mathbf{y} + \\lambda \\, \\mathbf{z}^k) \\;,$$\n",
    "\n",
    "via conjugate gradient. In the expression, $k$ is the index of the stage (iteration),\n",
    " $\\mathbf{z}^k = \\mathrm{ResNet}(\\mathbf{x}^{k})$ is the regularization\n",
    " (a denoiser implemented as a residual convolutional neural network), $\\mathbf{x}^k$ is the output\n",
    "  of the previous stage, $\\lambda > 0$\n",
    "  is a learned regularization parameter, and $\\mathbf{I}$ is the identity operator.\n",
    "  The output of the final stage is the set of reconstructed images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb529d",
   "metadata": {},
   "source": [
    "## Constructing the Data Sets\n",
    "\n",
    "Machine learning processing for images in Flax assumes the following data shape: $(K, H, W, C)$\n",
    "\n",
    "- $K$ is the number of image samples\n",
    "- $H$ is the height of the images\n",
    "- $W$ is the width of the images\n",
    "- $C$ is the number of channels of the images (e.g. 1 for grayscale images, 3 for color images)\n",
    "\n",
    "**Reformat the training data to have the expected Flax shape.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1561ac",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foam_collection = foam_collection.reshape((nfoams, N, N, 1))\n",
    "sino_collection = sino_collection.reshape((nfoams, n_projection, N, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c1ecbf",
   "metadata": {},
   "source": [
    "SCICO passes the data to the ML models as a dictionary, with the `image` key to define the input and the `label` key to define the expected output. In other words, (`image`, `label`) define the pair needed for supervised training.\n",
    "\n",
    "**Construct training and testing partitions for the CT reconstuction problem**. Use the first 16 images for training and the rest for testing. Remember that in the CT problem you want to reconstruct images from sinograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02327e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = {\"image\": sino_collection[:16], \"label\": foam_collection[:16]}\n",
    "test_ds = {\"image\": sino_collection[16:], \"label\": foam_collection[16:]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c57489e",
   "metadata": {},
   "source": [
    "## Configuring the ML model and its training\n",
    "\n",
    "SCICO configures the training via a dictionary too. An example of a configuration dictionary with the corresponding definitions is shown next.\n",
    "\n",
    "Run the next cell to build the configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1d1b3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dconf = {\n",
    "    \"seed\": 100, # Seed for random generation\n",
    "    \"depth\": 2, # Number of layers (=iterations) in the unrolled ML model\n",
    "    \"num_filters\": 16, # Number of filters in the denoiser\n",
    "    \"block_depth\": 3, # Number of layers in the denoiser\n",
    "    \"opt_type\": \"ADAM\", # Optimization (other available options: SGD, ADAMW)\n",
    "    \"batch_size\": 8, # Number of samples to include in each batch\n",
    "    \"num_epochs\": 50, # Number of training epochs\n",
    "    \"base_learning_rate\": 1e-2, # Base learning rate\n",
    "    \"warmup_epochs\": 0, # Iterations to reach the base learning rate (if a scheduler is specified)\n",
    "    \"num_train_steps\": -1, # Number of training steps, (if -1 train based on epochs specification)\n",
    "    \"steps_per_eval\": -1, # Number of steps in testing, (if -1 eval over all the testing set)\n",
    "    \"log_every_steps\": 5, # Frequency of reporting training stats, given in units of training steps\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8541e",
   "metadata": {},
   "source": [
    "## Constructing the ML model\n",
    "\n",
    "SCICO ML functionality is based on FLAX (see https://flax.readthedocs.io/en/latest/overview.html). Frequently used models are provided in SCICO.\n",
    "\n",
    "Run the next cell to import the Flax functionality in SCICO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff28a41",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scico import flax as sflax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e40d0fa",
   "metadata": {},
   "source": [
    "**Look how to construct a MoDL model in the SCICO documentation and construct it.** Use the parameters already defined, but use 1 as depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56838067",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "channels = train_ds[\"image\"].shape[-1]\n",
    "model = sflax.MoDLNet(\n",
    "        operator=A,\n",
    "        depth=1,\n",
    "        channels=channels,\n",
    "        num_filters=dconf[\"num_filters\"],\n",
    "        block_depth=dconf[\"block_depth\"],\n",
    "        cg_iter=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9e7ca",
   "metadata": {},
   "source": [
    "You are done with part 2. Please report back in the Webex chat: **done with part 2**.\n",
    "\n",
    "While you wait for others to finish, explore other ML models available in SCICO.\n",
    "\n",
    "ðŸ›‘ **PAUSE HERE** ðŸ›‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ace1d",
   "metadata": {},
   "source": [
    "# Training the MoDL model\n",
    "\n",
    "$\\lambda$, the regularization parameter in MoDL, is also learned in the training process. However, it is important that it keeps being a positive multiplier.\n",
    "\n",
    "Run the next cell to build the structure necessary to assure that the training will respect such constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36fcbb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from scico.flax.train.train import construct_traversal, clip_positive\n",
    "\n",
    "lmbdatrav = construct_traversal(\"lmbda\") # Functionality to track parameter to constraint inside model\n",
    "lmbdapos = partial(\n",
    "    clip_positive, # Type of constraint to apply, here positivity constraint\n",
    "    traversal=lmbdatrav,\n",
    "    minval=5e-4, # Minimum value to accept when enforcing the positivity constraint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc09f47",
   "metadata": {},
   "source": [
    "Now that we have all the structures needed for training: a data set, a model, and parameter constraints, we can use SCICO to train the model.\n",
    "\n",
    "All the training in SCICO is carried out through the `train_and_evaluate` function. The following cell shows how to pass the necessary information to that function. It uses an MSE loss function as minimization criterion by default. Look into the documentation and compare with the arguments provided.\n",
    "\n",
    "Run the next cell to train the model for the number of epochs specified. Check the output being produced. It corresponds, first, to the description of the model architecture and parameter default initialization and, next, to the training statistics. If it is taking too long, you can try to train for less epochs or use less layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be75c3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "workdir = \"./modl_ct/\"\n",
    "\n",
    "start_time = time()\n",
    "modvar, stats_object = sflax.train_and_evaluate(\n",
    "    dconf, # Dictionary with training configuration\n",
    "    workdir, # Directory to store checkpoints\n",
    "    model, # Model to train\n",
    "    train_ds, # Data set for training (image-label dictionary)\n",
    "    test_ds, # Data set for testing (image-label dictionary)\n",
    "    post_lst=[lmbdapos], # Constraints to model parameters\n",
    "    checkpointing=True, # Checkpoint stats during training\n",
    "    log=True # Display training messages and statistics\n",
    ")\n",
    "time_train = time() - start_time\n",
    "print(f\"Time train [s]: {time_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab92d15",
   "metadata": {},
   "source": [
    "Run the next cell to plot the training statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9324914",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist = stats_object.history(transpose=True)\n",
    "fig, ax = plot.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "plot.plot(\n",
    "    np.vstack((hist.Train_Loss, hist.Eval_Loss)).T,\n",
    "    x=hist.Epoch,\n",
    "    ptyp=\"semilogy\",\n",
    "    title=\"Loss function\",\n",
    "    xlbl=\"Epoch\",\n",
    "    ylbl=\"Loss value\",\n",
    "    lgnd=(\"Train\", \"Test\"),\n",
    "    fig=fig,\n",
    "    ax=ax[0],\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "    np.vstack((hist.Train_SNR, hist.Eval_SNR)).T,\n",
    "    x=hist.Epoch,\n",
    "    title=\"Metric\",\n",
    "    xlbl=\"Epoch\",\n",
    "    ylbl=\"SNR (dB)\",\n",
    "    lgnd=(\"Train\", \"Test\"),\n",
    "    fig=fig,\n",
    "    ax=ax[1],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d0261",
   "metadata": {},
   "source": [
    "The MoDL architecture shares the parameters between the different iteration layers. The previous training was the initialization and used only one iteration. Now we can train the model with the specified depth.\n",
    "\n",
    "**Repeat the training process**, but this time use the configured depth, 10 cg iterations and initialize with the current model parameters. **Train** for a longer number of epochs. In addition, use the exponentially decaying learning rate function. You need to add a decay rate to the configuration dictionary. Use a decaying rate of 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918699d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scico.flax.train.train import create_exp_lr_schedule\n",
    "\n",
    "model.depth = dconf[\"depth\"]\n",
    "model.cg_iter = 10\n",
    "dconf[\"num_epochs\"] = 100\n",
    "dconf[\"lr_decay_rate\"] = 0.95\n",
    "\n",
    "workdir2 = workdir + \"iterated/\"\n",
    "\n",
    "start_time = time()\n",
    "modvar, stats_object = sflax.train_and_evaluate(\n",
    "    dconf, # Dictionary with training configuration\n",
    "    workdir2, # Directory to store checkpoints\n",
    "    model, # Model to train\n",
    "    train_ds, # Data set for training (image-label dictionary)\n",
    "    test_ds, # Data set for testing (image-label dictionary)\n",
    "    create_lr_schedule = create_exp_lr_schedule, # Exponentially decaying LR\n",
    "    post_lst=[lmbdapos], # Constraints to model parameters\n",
    "    variables0=modvar, # Model variables after initial training\n",
    "    checkpointing=True, # Checkpoint stats during training\n",
    "    log=True # Display training messages and statistics\n",
    ")\n",
    "time_train = time() - start_time\n",
    "print(f\"Time train [s]: {time_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01e157",
   "metadata": {},
   "source": [
    "Plot the training stats for this last training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b4029",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist = stats_object.history(transpose=True)\n",
    "fig, ax = plot.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "plot.plot(\n",
    "    np.vstack((hist.Train_Loss, hist.Eval_Loss)).T,\n",
    "    x=hist.Epoch,\n",
    "    ptyp=\"semilogy\",\n",
    "    title=\"Loss function\",\n",
    "    xlbl=\"Epoch\",\n",
    "    ylbl=\"Loss value\",\n",
    "    lgnd=(\"Train\", \"Test\"),\n",
    "    fig=fig,\n",
    "    ax=ax[0],\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "    np.vstack((hist.Train_SNR, hist.Eval_SNR)).T,\n",
    "    x=hist.Epoch,\n",
    "    title=\"Metric\",\n",
    "    xlbl=\"Epoch\",\n",
    "    ylbl=\"SNR (dB)\",\n",
    "    lgnd=(\"Train\", \"Test\"),\n",
    "    fig=fig,\n",
    "    ax=ax[1],\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b917908e",
   "metadata": {},
   "source": [
    "You are done with part 3. Please report back in the Webex chat: **done with part 3**.\n",
    "\n",
    "While you wait for others to finish, think of ways to improve the performance of MoDL for CT reconstruction.\n",
    "\n",
    "ðŸ›‘ **PAUSE HERE** ðŸ›‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea36a9",
   "metadata": {},
   "source": [
    "# Deploying a trained model\n",
    "\n",
    "If all you need to do is to apply a trained model, SCICO provides the `FlaxMap` class. This also allows you to connect trained models to other SCICO functionality.\n",
    "\n",
    "Run the next cell to see the trained model applied to the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda8599",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "fmap = sflax.FlaxMap(model, modvar)\n",
    "output = fmap(test_ds[\"image\"])\n",
    "time_eval = time() - start_time\n",
    "output = np.clip(output, a_min=0, a_max=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11547459",
   "metadata": {},
   "source": [
    "Use SCICO documentation to figure out how to compute SNR and MAE for the reconstructions obtained with MoDL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d77e682",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scico import metric\n",
    "snr_eval = metric.snr(test_ds[\"label\"], output)\n",
    "mae_eval = metric.mae(test_ds[\"label\"], output)\n",
    "print(f\"SNR [dB]: {snr_eval}\")\n",
    "print(f\"MAE: {mae_eval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974c3ee",
   "metadata": {},
   "source": [
    "Run the next cell to check one of the testing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0377d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(543)\n",
    "indx = np.random.randint(0, high=8)\n",
    "\n",
    "fig, ax = plot.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "plot.imview(test_ds[\"label\"][indx, ..., 0], title=\"Ground truth\", cbar=None, fig=fig, ax=ax[0])\n",
    "plot.imview(\n",
    "    test_ds[\"image\"][indx, ..., 0],\n",
    "    title=\"Sinogram\",\n",
    "    cbar=None,\n",
    "    fig=fig,\n",
    "    ax=ax[1],\n",
    ")\n",
    "plot.imview(\n",
    "    output[indx, ..., 0],\n",
    "    title=\"MoDL Reconstruction\\nSNR: %.2f (dB), MAE: %.3f\"\n",
    "    % (\n",
    "        metric.snr(test_ds[\"label\"][indx, ..., 0], output[indx, ..., 0]),\n",
    "        metric.mae(test_ds[\"label\"][indx, ..., 0], output[indx, ..., 0]),\n",
    "    ),\n",
    "    fig=fig,\n",
    "    ax=ax[2],\n",
    ")\n",
    "divider = make_axes_locatable(ax[2])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\n",
    "fig.colorbar(ax[2].get_images()[0], cax=cax, label=\"arbitrary units\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2716a5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This tutorial has shown how to set up\n",
    "a SCICO pipeline for performing computed tomography (CT) reconstruction using data-driven priors. In doing so, it has demonstrated SCICO functionality to build and train ML models for imaging problems. The functionality is based on FLAX and provides a  straightforward pipeline for other ML applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81337506",
   "metadata": {},
   "source": [
    "You are done with this tutorial! Please report back in the Webex chat: **done with the CNN tutorial**.\n",
    "\n",
    "While you wait for others to finish, you could think of similar problems you may want to solve with SCICO. We want to discuss other uses and help you deploy SCICO on suitable applications."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
